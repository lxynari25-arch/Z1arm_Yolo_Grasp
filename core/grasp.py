# å®žçŽ°çœ¼åœ¨æ‰‹ä¸Šçš„æŠ“å–æµç¨‹
import signal
import sys
import time
import argparse
import numpy as np
from ultralytics import YOLO
from realsense2 import *
from wrap_z1 import z1_arm
from calibrate import Calibration
from pathlib import Path
from utils import *
from config_manager import config_manager
from move import *
from b2_message import *

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]

# å…¨å±€å˜é‡ï¼Œç”¨äºŽä¿¡å·å¤„ç†
robot = None

def signal_handler(signum, frame):
    """å¤„ç†Ctrl+Cä¿¡å·ï¼Œä¼˜é›…åœ°å…³é—­ç¨‹åº"""
    print("\næ£€æµ‹åˆ°Ctrl+Cï¼Œæ­£åœ¨è¿”å›žåˆå§‹ä½ç½®...")
    if robot is not None:
        try:
            robot.move_init_pose()
            print("æœºæ¢°è‡‚å·²è¿”å›žåˆå§‹ä½ç½®")
        except Exception as e:
            print(f"é”™è¯¯ï¼šç§»åŠ¨æœºæ¢°è‡‚æ—¶å‡ºçŽ°å¼‚å¸¸ {e}")
    print("ç¨‹åºå·²é€€å‡º")
    sys.exit(0)

def create_parser():
    print_and_write(None, "------------------------config------------------------", 33)
    
    # æ˜¾ç¤ºå¯ç”¨çš„é…ç½®
    available_configs = config_manager.list_available_configs()
    print_and_write(None, "å¯ç”¨çš„æ¨¡åž‹é…ç½®:", 33)
    for config_name, description in available_configs.items():
        print_and_write(None, f"  - {config_name}: {description}", 33)
    print_and_write(None, "")
    
    parser = argparse.ArgumentParser(description='GRASP')                                      
    parser.add_argument('--date', type=str, default='test', help='calibration file')
    parser.add_argument('--debug', type=bool, default=True, help='enable debug mode to save images')
    parser.add_argument('--mode', type=int, default=1, choices=[1, 2],
                        help='è¿åŠ¨æ¨¡å¼: 1=å›ºå®šå§¿æ€, 2=ä½¿ç”¨é¢„è®¾å§¿æ€æ‹ç…§')
    parser.add_argument('--config', type=str, default=None,
                        help='æ¨¡åž‹é…ç½®åç§°ï¼ˆå¦‚: yolo26n-objv1-150, yolo11n-object365-penç­‰ï¼‰')
    return parser

if __name__ == "__main__":
    signal.signal(signal.SIGINT, signal_handler)

    parser = create_parser()
    args = parser.parse_args()
    
    # å¤„ç†é…ç½®åˆ‡æ¢
    if args.config:
        try:
            config_manager.set_current_config(args.config)
            print_and_write(None, f"å·²åˆ‡æ¢åˆ°é…ç½®: {args.config}", 32)
        except ValueError as e:
            print_and_write(None, f"é”™è¯¯: {e}", 31)
            sys.exit(1)
    
    # æ‰“å°å½“å‰é…ç½®ä¿¡æ¯
    config_manager.print_current_config()
    
    # ä»Žé…ç½®æ–‡ä»¶èŽ·å–æ¨¡åž‹è·¯å¾„
    yolo_path = config_manager.get_model_path()
    object_class_id = config_manager.get_class_id()
    object_name = config_manager.get_object_name()
    
    # åˆå§‹åŒ–æ¨¡åž‹å’Œæœºå™¨äºº
    print("åˆå§‹åŒ–æ¨¡åž‹å’Œæœºæ¢°è‡‚")
    model = YOLO(yolo_path, verbose=False)

    robot = z1_arm()
    realcam = Realsense2(0, 640, 480, 30)

    # èŽ·å–æ ‡å®šæ•°æ®
    cal_data = np.load(f'{ROOT}/../output/imgSave/{args.date}/data.npy', allow_pickle=True).item()
    RT_cam2gri = cal_data['RT_cam2gripper']
    RT_obj2cam = np.identity(4)
    RT_gri2base = np.identity(4)

    # ç§»åŠ¨åˆ°åˆå§‹ä½å§¿
    if args.mode == 1:
        robot.move_init_pose()   
    elif args.mode == 2:
        robot.labelrun("upcamera")
    time.sleep(2)

    # æ¯æ¬¡å®Œæ•´æŠ“å–æµç¨‹éƒ½ç”±ç”¨æˆ·æŒ‰å›žè½¦è§¦å‘ï¼šå…ˆç­‰å¾…æŒ‰å›žè½¦ï¼Œå†è¿›å…¥ç›¸æœºè¯†åˆ«å¾ªçŽ¯ï¼›
    # è¯†åˆ«åˆ°ç›®æ ‡å¹¶å®Œæˆä¸€æ¬¡æŠ“å–åŽè¿”å›žç­‰å¾…ä¸‹ä¸€æ¬¡æŒ‰å›žè½¦ã€‚
    while True:
        input("æŒ‰å›žè½¦å¼€å§‹è¯†åˆ«ï¼š")
        flush_camera_buffer(realcam, flush_count=30)
        grasp_success = 0
        move_command_middle = 0
        move_command_end = 0
        for img in realcam:
            rgb_img, depth_vis_img, depth_frame = img
            if len(np.array(rgb_img).shape) == 3:
                # ç½®ä¿¡åº¦é˜ˆå€¼ä¿®æ”¹äº†ï¼ï¼ï¼
                results = model(rgb_img, conf = 0.08, verbose=False)[0]
                img_xy, names, confidence = cal_center(rgb_img, results.names, results.boxes.data.tolist(), object_class_id)
                if img_xy:
                    print("**************************è¯†åˆ«åˆ°å¯å¤¹ä½çš„ç‰©ä½“ï¼ï¼ï¼**************************")
                    print(f"img_xy: {img_xy}")
                    real_xyz = realcam.pixel2point(img_xy)
                    print(f"ç›¸æœºåæ ‡ç³»ä¸‹åæ ‡: {real_xyz}")
                    print(f"è¯†åˆ«çš„ç‰©ä½“åç§°ä¸ºï¼š{names}ï¼Œç½®ä¿¡åº¦ä¸ºï¼š{confidence}")

                    RT_obj2cam[:3, 3] = np.array(real_xyz)
                    RT_gri2base = Calibration.Cartesian2Homogeneous(robot.get_current_pose())
                    RT_obj2base = RT_gri2base.dot(RT_cam2gri.dot(RT_obj2cam))
                    print(f"RT_obj2base: {RT_obj2base}")

                    if args.debug:
                        save_debug_images(rgb_img, depth_vis_img, depth_frame)

                    # æ ¹æ®è¿åŠ¨æ¨¡å¼è®¾ç½® r, p, y
                    x, y, z = RT_obj2base[:3, 3]

                    # é™¤åŽ»å¼‚å¸¸xå€¼
                    if x >= 0.8 or x <= 0.3:
                        print(f"------å‡ºçŽ°å¼‚å¸¸xå€¼ï¼š{x}--------")
                        break
                    
                    print(f"ç›®æ ‡ç‰©ä½“åœ¨åŸºåº§åæ ‡ç³»ä¸‹ä½ç½®: x={x}, y={y}, z={z}")
                    if args.mode == 1:
                        # å›ºå®šå§¿æ€æ¨¡å¼
                        r, p, yaw = 0.0, 0.0, 0.0
                    elif args.mode == 2:
                        r, p, yaw = 0.0, 1.0, 0.0

                    move_command = np.array([r, p, yaw, x, y, z], dtype=float)
                    print(f"[è¿åŠ¨æ‰§è¡Œ] æ‰§è¡Œmovejè¿åŠ¨æŒ‡ä»¤ï¼š")
                    print(f"å®Œæ•´æŒ‡ä»¤: {move_command}")

                    # ä¸­é—´
                    move_command_middle = np.array([r, p, yaw, x-0.15, y, z-0.055], dtype=float)
                    print(f"ä¸­é—´ç‚¹åæ ‡ï¼š{x-0.15, y, z-0.055}")
                    robot(move_command_middle, -1)
                    time.sleep(1)
                    # æœ«ç«¯
                    move_command_end = np.array([r, p, yaw, x-0.08, y, z-0.055], dtype=float)
                    print(f"æœ«ç«¯ç‚¹åæ ‡ï¼š{x-0.08, y, z-0.055}")
                    robot(move_command_end, -1)
                    time.sleep(1)
                    # çˆªå­é—­åˆï¼ï¼ï¼
                    robot(move_command_end, 1)
                    if args.mode == 1:
                        robot.move_init_pose()
                    elif args.mode == 2:
                        robot.labelrun("upcamera")

                    # å®Œæˆä¸€æ¬¡æŠ“å–åŽé€€å‡ºå¸§å¾ªçŽ¯ï¼Œå›žåˆ°ç­‰å¾…æŒ‰å›žè½¦
                    grasp_success = 1
                    break
        
        if grasp_success == 0:
            continue

        # 1. åˆ›å»ºä¸€ä¸ªé˜Ÿåˆ—
        q = Queue()

        # 2. åˆ›å»ºç›‘å¬å®žä¾‹
        listener = RobotDogListener()
        # 3. å¯åŠ¨å­è¿›ç¨‹ï¼Œåªä¼ é˜Ÿåˆ—ï¼ˆè¶…çº§å¹²å‡€ï¼‰
        p = multiprocessing.Process(target=listener.start, args=(q,), daemon=True)
        p.start()
        print("ðŸš€ ä¸»è¿›ç¨‹è¿è¡Œä¸­ï¼Œç­‰å¾…æœºå™¨ç‹—åˆ°è¾¾...")

        # å‘é€ POST è¯·æ±‚ï¼Œè®©ç‹—ç§»åŠ¨
        print("æ­£åœ¨å‘æœºå™¨ç‹—å‘é€ç§»åŠ¨å‘½ä»¤...")
        response = requests.post(url, data=json.dumps(payload), headers=headers)

        # ç›‘å¬å­è¿›ç¨‹çš„æ¶ˆæ¯ï¼Œåˆ¤æ–­æœºå™¨ç‹—æ˜¯å¦åˆ°è¾¾ï¼ï¼ï¼
        while True:
            if not q.empty():
                msg = q.get()
                if msg == "arrived":
                    move_command_end[3] = move_command_end[3] + 0.05
                    move_command_end[5] = move_command_end[5] + 0.08
                    robot(move_command_end, 1)
                    time.sleep(1)
                    robot(move_command_end, -1)
                    time.sleep(1)
                    # robot(move_command_middle, -1)
                    if args.mode == 1:
                        robot.move_init_pose()
                    elif args.mode == 2:
                        robot.labelrun("upcamera")
                    break
                    
            asyncio.run(asyncio.sleep(0.1))

    print("End.")